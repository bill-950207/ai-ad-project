/**
 * ì”¬ë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ìƒì„± API (Vidu Q2 / Kling O1ìš©)
 *
 * POST: ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ì”¬ì˜ ê°œë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 * - ê° ì”¬ì€ ê°œë³„ ì˜ìƒìœ¼ë¡œ ìƒì„±ëœ í›„ ë‚˜ì¤‘ì— í•©ì³ì§
 * - ì”¬ë³„ë¡œ ë‹¤ë¥¸ ë°°ê²½, ì¡°ëª…, ì¹´ë©”ë¼ ì•µê¸€ ë“± ê°œë³„ ì„¤ì • ì§€ì›
 * - ëª¨ë“  ì”¬ì˜ í†¤ì•¤ë§¤ë„ˆ/ìƒ‰ê°ì´ ì¼ê´€ë˜ì–´ì•¼ ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„± ê°€ëŠ¥
 * - ì‚¬ëŒ/ì–¼êµ´ ì œì™¸, ì œí’ˆ ì¤‘ì‹¬ ê´‘ê³ 
 */

import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@/lib/supabase/server'
import { GoogleGenAI, GenerateContentConfig, ThinkingLevel, Type } from '@google/genai'

// Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const genAI = new GoogleGenAI({
  apiKey: process.env.GOOGLE_AI_API_KEY!,
})

const MODEL_NAME = 'gemini-3-flash-preview'

// ì”¬ë³„ ê´‘ê³  ìš”ì†Œ
interface SceneElementOptions {
  background: string
  mood: string
  cameraAngle: string
  productPlacement: string
  lighting: string
  colorTone: string
}

// ë ˆê±°ì‹œ í˜¸í™˜ìš© (ì „ì²´ ìš”ì†Œ)
interface ScenarioElements {
  background: string
  mood: string
  cameraAngle: string
  productPlacement: string
  lighting: string
  colorTone: string
}

interface GenerateMultiSceneRequest {
  productName: string
  productDescription?: string | null
  productImageUrl?: string | null
  sellingPoints?: string[] | null
  sceneElements?: SceneElementOptions[]  // ì”¬ë³„ ê´‘ê³  ìš”ì†Œ (ì‹ ê·œ)
  scenarioElements?: ScenarioElements  // ë ˆê±°ì‹œ í˜¸í™˜ (ì „ì²´ ìš”ì†Œ)
  overallMood?: string  // ì „ì²´ ë¶„ìœ„ê¸°
  scenarioDescription?: string
  sceneCount?: number
  totalDuration?: number
}

interface SceneOutput {
  index: number
  scenePrompt: string
  duration: number
  movementAmplitude: 'auto' | 'small' | 'medium' | 'large'
}

/**
 * URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ base64ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
 */
async function fetchImageAsBase64(url: string): Promise<{ base64: string; mimeType: string } | null> {
  try {
    const response = await fetch(url)
    if (!response.ok) return null

    const contentType = response.headers.get('content-type') || 'image/jpeg'
    const buffer = await response.arrayBuffer()
    const base64 = Buffer.from(buffer).toString('base64')

    return { base64, mimeType: contentType }
  } catch (error) {
    console.error('ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜:', error)
    return null
  }
}

export async function POST(request: NextRequest) {
  try {
    const supabase = await createClient()
    const { data: { user }, error: authError } = await supabase.auth.getUser()

    if (authError || !user) {
      return NextResponse.json(
        { error: 'Unauthorized' },
        { status: 401 }
      )
    }

    const body: GenerateMultiSceneRequest = await request.json()
    const {
      productName,
      productDescription,
      productImageUrl,
      sellingPoints,
      sceneElements,  // ì”¬ë³„ ìš”ì†Œ (ì‹ ê·œ)
      scenarioElements,  // ë ˆê±°ì‹œ í˜¸í™˜
      overallMood,
      scenarioDescription,
      sceneCount = 3,
      totalDuration = 15,
    } = body

    // sceneElements ë˜ëŠ” scenarioElements ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜
    if (!productName || (!sceneElements && !scenarioElements)) {
      return NextResponse.json(
        { error: 'Missing required fields' },
        { status: 400 }
      )
    }

    // ì”¬ë³„ ìš”ì†Œê°€ ì—†ìœ¼ë©´ ë ˆê±°ì‹œ ìš”ì†Œë¡œ ëª¨ë“  ì”¬ì— ë™ì¼í•˜ê²Œ ì ìš©
    const resolvedSceneElements: SceneElementOptions[] = sceneElements
      ? sceneElements
      : Array(sceneCount).fill(scenarioElements)

    // ê° ì”¬ì˜ í‰ê·  ê¸¸ì´ ê³„ì‚°
    const avgDuration = Math.min(10, Math.max(3, Math.floor(totalDuration / (sceneCount - 1))))

    // ì œí’ˆ ì´ë¯¸ì§€ base64 ë³€í™˜
    let productImageData: { base64: string; mimeType: string } | null = null
    if (productImageUrl) {
      productImageData = await fetchImageAsBase64(productImageUrl)
    }

    const prompt = buildMultiScenePrompt(
      productName,
      productDescription,
      sellingPoints,
      resolvedSceneElements,
      overallMood,
      scenarioDescription,
      sceneCount,
      avgDuration
    )

    const config: GenerateContentConfig = {
      thinkingConfig: {
        thinkingLevel: ThinkingLevel.LOW,
      },
      responseMimeType: 'application/json',
      responseSchema: {
        type: Type.OBJECT,
        required: ['scenes', 'visualStyle'],
        properties: {
          scenes: {
            type: Type.ARRAY,
            description: 'ìƒì„±ëœ ì”¬ë³„ ê°œë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ë°°ì—´',
            items: {
              type: Type.OBJECT,
              required: ['index', 'scenePrompt', 'duration', 'movementAmplitude'],
              properties: {
                index: {
                  type: Type.NUMBER,
                  description: 'ì”¬ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)',
                },
                scenePrompt: {
                  type: Type.STRING,
                  description: 'ì´ ì”¬ì˜ ê°œë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ - ëª¨ì…˜ í¬í•¨, ì‚¬ëŒ ì œì™¸, 40-60ë‹¨ì–´ (ì˜ì–´)',
                },
                duration: {
                  type: Type.NUMBER,
                  description: 'ì”¬ ì˜ìƒ ê¸¸ì´ (ì´ˆ, 3-8)',
                },
                movementAmplitude: {
                  type: Type.STRING,
                  description: 'ì¹´ë©”ë¼/ëª¨ì…˜ ê°•ë„',
                  enum: ['auto', 'small', 'medium', 'large'],
                },
              },
            },
          },
          visualStyle: {
            type: Type.STRING,
            description: 'ëª¨ë“  ì”¬ì— ì ìš©ëœ ê³µí†µ ì‹œê°ì  ìŠ¤íƒ€ì¼ ìš”ì•½ (ì˜ì–´)',
          },
          narrativeFlow: {
            type: Type.STRING,
            description: 'ì „ì²´ ì˜ìƒì˜ ë‚´ëŸ¬í‹°ë¸Œ íë¦„ ì„¤ëª… (í•œêµ­ì–´)',
          },
        },
      },
    }

    // ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  êµ¬ì„±
    const parts: Array<{ text: string } | { inlineData: { mimeType: string; data: string } }> = []

    if (productImageData) {
      parts.push({
        inlineData: {
          mimeType: productImageData.mimeType,
          data: productImageData.base64,
        },
      })
    }

    parts.push({ text: prompt })

    const response = await genAI.models.generateContent({
      model: MODEL_NAME,
      contents: [{ role: 'user', parts }],
      config,
    })

    const responseText = response.text || ''
    const result = JSON.parse(responseText) as {
      scenes: SceneOutput[]
      visualStyle?: string
      narrativeFlow?: string
    }

    return NextResponse.json({
      scenes: result.scenes,
      visualStyle: result.visualStyle,
      narrativeFlow: result.narrativeFlow,
    })
  } catch (error) {
    console.error('ë©€í‹°ì”¬ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì˜¤ë¥˜:', error)
    return NextResponse.json(
      { error: 'Failed to generate multi-scene scenario' },
      { status: 500 }
    )
  }
}

function buildMultiScenePrompt(
  productName: string,
  productDescription: string | null | undefined,
  sellingPoints: string[] | null | undefined,
  sceneElements: SceneElementOptions[],
  overallMood: string | undefined,
  scenarioDescription: string | undefined,
  sceneCount: number,
  avgDuration: number
): string {
  // ì…€ë§ í¬ì¸íŠ¸ í¬ë§·íŒ…
  const sellingPointsText = sellingPoints && sellingPoints.length > 0
    ? sellingPoints.map((p, i) => `  ${i + 1}. ${p}`).join('\n')
    : 'Not provided'

  // ì”¬ë³„ ìš”ì†Œ ì„¤ëª… ìƒì„±
  const sceneElementsDescription = sceneElements.map((elem, idx) => `
=== SCENE ${idx + 1} SPECIFIC ELEMENTS ===
- Background: ${elem.background || 'Clean seamless backdrop'}
- Mood: ${elem.mood || overallMood || 'Premium commercial'}
- Camera Angle: ${elem.cameraAngle || 'Cinematic'}
- Product Placement: ${elem.productPlacement || 'Center hero'}
- Lighting: ${elem.lighting || 'Professional even lighting'}
- Color Tone: ${elem.colorTone || 'Natural'}
`).join('\n')

  // ëŒ€í‘œ ìƒ‰ìƒ í†¤ (ì¼ê´€ì„± ìœ ì§€ìš©)
  const primaryColorTone = sceneElements[0]?.colorTone || overallMood || 'cinematic'

  return `You are an expert advertising video director creating a PREMIUM AD CAMPAIGN with ${sceneCount} scenes.

ğŸ¬ GOAL: Create ${sceneCount} SCENES where EACH scene uses its OWN specific elements listed below.
IMPORTANT: Each scene has DIFFERENT settings - USE the specific elements for EACH scene.

=== CRITICAL RULES ===
âŒ ABSOLUTELY NO PEOPLE:
- NO humans, faces, hands, body parts, silhouettes
- ONLY the product, objects, environment, and natural elements

âœ… OUTPUT LANGUAGE:
- scenePrompt: English only (for AI video generation)
- visualStyle: English (shared style description)
- narrativeFlow: Korean (for user)

â­ MUST REFERENCE THE ATTACHED IMAGE:
- Each scenePrompt MUST start with "The product shown in the attached image" or similar
- This ensures the AI uses the EXACT product appearance

=== PRODUCT INFORMATION ===
Product Name: ${productName}
Product Description: ${productDescription || 'Not provided'}
Product Selling Points:
${sellingPointsText}
Product Image: [ATTACHED - This is the PRODUCT to feature]

=== OVERALL MOOD (ì „ì²´ í†¤ì•¤ë§¤ë„ˆ) ===
${overallMood || 'Premium commercial style'}
${scenarioDescription ? `Scenario Description: ${scenarioDescription}` : ''}

=== âš ï¸ SCENE-SPECIFIC ELEMENTS (ê° ì”¬ë³„ ê°œë³„ ì„¤ì • - ë§¤ìš° ì¤‘ìš”!) ===
EACH SCENE HAS DIFFERENT SETTINGS. You MUST use the specific elements for each scene:

${sceneElementsDescription}

âš ï¸ IMPORTANT:
- Scene 1 MUST use Scene 1's background, lighting, camera angle, etc.
- Scene 2 MUST use Scene 2's settings, which may be DIFFERENT from Scene 1
- And so on for all scenes
- This creates visual variety while maintaining the overall mood

=== ğŸ¨ VISUAL CONSISTENCY (í†¤ì•¤ë§¤ë„ˆ í†µì¼) ===
While scenes have DIFFERENT settings, they should share:
1. **Same overall mood/feeling**: ${overallMood || 'Premium commercial'}
2. **Quality level**: All scenes should feel like same premium campaign
3. **Transition flow**: Scenes should connect naturally

=== SCENE PROMPT STRUCTURE (50-80 words) ===
Each scenePrompt MUST:
1. START by identifying the product: "The ${productName} shown in the attached image"
2. Use THAT SCENE's specific background, lighting, camera angle from the elements above
3. Be UNIQUE based on its specific elements
4. End with: "[THAT SCENE'S COLOR TONE] tones, cinematic lighting, photorealistic, 4K"

=== VIDU Q2 VIDEO AI RULES ===
- NO text/label rendering expectations
- Simple surface descriptions (no contradictions)
- ALL camera movements MUST include "slowly"
- Maximum 2 visual effects per scene

=== ğŸšï¸ MOVEMENT AMPLITUDE ===
- "small": Static, elegant (close-up, detail shots)
- "medium": Moderate camera movement (recommended)
- "large": Dynamic, energetic (opening/closing)

=== DURATION ===
Each scene: ${avgDuration} seconds (range: 3-8)

Generate ${sceneCount} CONNECTED scene prompts for "${productName}".
Create a unique and creative narrative flow that best highlights this specific product's characteristics.

âš ï¸ MANDATORY:
1. Each scene MUST use ITS OWN specific elements from the list above
2. Scene 1 uses Scene 1 elements, Scene 2 uses Scene 2 elements, etc.
3. Scenes should flow together as one story while having VISUAL VARIETY
4. NO REAL PEOPLE - only product and environment
5. Each prompt ends with that scene's color tone + "cinematic lighting, photorealistic, 4K"
6. End visualStyle with: "Overall mood: ${primaryColorTone}, premium commercial quality"`
}
