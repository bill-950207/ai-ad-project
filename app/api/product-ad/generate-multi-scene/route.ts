/**
 * ì”¬ë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ìƒì„± API (Vidu Q2 / Kling O1ìš©)
 *
 * POST: ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ì”¬ì˜ ê°œë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
 * - ê° ì”¬ì€ ê°œë³„ ì˜ìƒìœ¼ë¡œ ìƒì„±ëœ í›„ ë‚˜ì¤‘ì— í•©ì³ì§
 * - ì”¬ë³„ë¡œ ë‹¤ë¥¸ ë°°ê²½, ì¡°ëª…, ì¹´ë©”ë¼ ì•µê¸€ ë“± ê°œë³„ ì„¤ì • ì§€ì›
 * - ëª¨ë“  ì”¬ì˜ í†¤ì•¤ë§¤ë„ˆ/ìƒ‰ê°ì´ ì¼ê´€ë˜ì–´ì•¼ ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„± ê°€ëŠ¥
 * - ì œí’ˆ ì¤‘ì‹¬ ê´‘ê³  (ì†/ì‹ ì²´ í—ˆìš©, ì–¼êµ´ í´ë¡œì¦ˆì—… ì œì™¸)
 */

import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@/lib/supabase/server'
import { GoogleGenAI, GenerateContentConfig, ThinkingLevel, Type } from '@google/genai'

// Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const genAI = new GoogleGenAI({
  apiKey: process.env.GOOGLE_AI_API_KEY!,
})

const MODEL_NAME = 'gemini-3-flash-preview'

// ì”¬ë³„ ê´‘ê³  ìš”ì†Œ
interface SceneElementOptions {
  background: string
  mood: string
  cameraAngle: string
  productPlacement: string
  lighting: string
  colorTone: string
}

// ë ˆê±°ì‹œ í˜¸í™˜ìš© (ì „ì²´ ìš”ì†Œ)
interface ScenarioElements {
  background: string
  mood: string
  cameraAngle: string
  productPlacement: string
  lighting: string
  colorTone: string
}

interface GenerateMultiSceneRequest {
  productName: string
  productDescription?: string | null
  productImageUrl?: string | null
  sellingPoints?: string[] | null
  sceneElements?: SceneElementOptions[]  // ì”¬ë³„ ê´‘ê³  ìš”ì†Œ (ì‹ ê·œ)
  scenarioElements?: ScenarioElements  // ë ˆê±°ì‹œ í˜¸í™˜ (ì „ì²´ ìš”ì†Œ)
  overallMood?: string  // ì „ì²´ ë¶„ìœ„ê¸°
  scenarioDescription?: string
  sceneCount?: number
  totalDuration?: number
}

interface SceneOutput {
  index: number
  scenePrompt: string
  duration: number
  movementAmplitude: 'auto' | 'small' | 'medium' | 'large'
}

/**
 * URLì—ì„œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ base64ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
 */
async function fetchImageAsBase64(url: string): Promise<{ base64: string; mimeType: string } | null> {
  try {
    const response = await fetch(url)
    if (!response.ok) return null

    const contentType = response.headers.get('content-type') || 'image/jpeg'
    const buffer = await response.arrayBuffer()
    const base64 = Buffer.from(buffer).toString('base64')

    return { base64, mimeType: contentType }
  } catch (error) {
    console.error('ì´ë¯¸ì§€ ë¡œë“œ ì˜¤ë¥˜:', error)
    return null
  }
}

export async function POST(request: NextRequest) {
  try {
    const supabase = await createClient()
    const { data: { user }, error: authError } = await supabase.auth.getUser()

    if (authError || !user) {
      return NextResponse.json(
        { error: 'Unauthorized' },
        { status: 401 }
      )
    }

    const body: GenerateMultiSceneRequest = await request.json()
    const {
      productName,
      productDescription,
      productImageUrl,
      sellingPoints,
      sceneElements,  // ì”¬ë³„ ìš”ì†Œ (ì‹ ê·œ)
      scenarioElements,  // ë ˆê±°ì‹œ í˜¸í™˜
      overallMood,
      scenarioDescription,
      sceneCount = 3,
      totalDuration = 15,
    } = body

    // sceneElements ë˜ëŠ” scenarioElements ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜
    if (!productName || (!sceneElements && !scenarioElements)) {
      return NextResponse.json(
        { error: 'Missing required fields' },
        { status: 400 }
      )
    }

    // ì”¬ë³„ ìš”ì†Œê°€ ì—†ìœ¼ë©´ ë ˆê±°ì‹œ ìš”ì†Œë¡œ ëª¨ë“  ì”¬ì— ë™ì¼í•˜ê²Œ ì ìš©
    const resolvedSceneElements: SceneElementOptions[] = sceneElements
      ? sceneElements
      : Array(sceneCount).fill(scenarioElements)

    // ê° ì”¬ì˜ í‰ê·  ê¸¸ì´ ê³„ì‚°
    const avgDuration = Math.min(10, Math.max(3, Math.floor(totalDuration / (sceneCount - 1))))

    // ì œí’ˆ ì´ë¯¸ì§€ base64 ë³€í™˜
    let productImageData: { base64: string; mimeType: string } | null = null
    if (productImageUrl) {
      productImageData = await fetchImageAsBase64(productImageUrl)
    }

    const prompt = buildMultiScenePrompt(
      productName,
      productDescription,
      sellingPoints,
      resolvedSceneElements,
      overallMood,
      scenarioDescription,
      sceneCount,
      avgDuration
    )

    const config: GenerateContentConfig = {
      thinkingConfig: {
        thinkingLevel: ThinkingLevel.LOW,
      },
      responseMimeType: 'application/json',
      responseSchema: {
        type: Type.OBJECT,
        required: ['scenes', 'visualStyle'],
        properties: {
          scenes: {
            type: Type.ARRAY,
            description: 'ìƒì„±ëœ ì”¬ë³„ ê°œë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ë°°ì—´',
            items: {
              type: Type.OBJECT,
              required: ['index', 'scenePrompt', 'duration', 'movementAmplitude'],
              properties: {
                index: {
                  type: Type.NUMBER,
                  description: 'ì”¬ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)',
                },
                scenePrompt: {
                  type: Type.STRING,
                  description: 'ì´ ì”¬ì˜ ê°œë³„ ì˜ìƒ í”„ë¡¬í”„íŠ¸ - ëª¨ì…˜ í¬í•¨, 50-100 words (ì˜ì–´). ì†/ì‹ ì²´ í—ˆìš©, ì–¼êµ´ í´ë¡œì¦ˆì—… ì œì™¸',
                },
                duration: {
                  type: Type.NUMBER,
                  description: 'ì”¬ ì˜ìƒ ê¸¸ì´ (ì´ˆ, 3-8)',
                },
                movementAmplitude: {
                  type: Type.STRING,
                  description: 'ì¹´ë©”ë¼/ëª¨ì…˜ ê°•ë„',
                  enum: ['auto', 'small', 'medium', 'large'],
                },
              },
            },
          },
          visualStyle: {
            type: Type.STRING,
            description: 'ëª¨ë“  ì”¬ì— ì ìš©ëœ ê³µí†µ ì‹œê°ì  ìŠ¤íƒ€ì¼ ìš”ì•½ (ì˜ì–´)',
          },
          narrativeFlow: {
            type: Type.STRING,
            description: 'ì „ì²´ ì˜ìƒì˜ ë‚´ëŸ¬í‹°ë¸Œ íë¦„ ì„¤ëª… (í•œêµ­ì–´)',
          },
        },
      },
    }

    // ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  êµ¬ì„±
    const parts: Array<{ text: string } | { inlineData: { mimeType: string; data: string } }> = []

    if (productImageData) {
      parts.push({
        inlineData: {
          mimeType: productImageData.mimeType,
          data: productImageData.base64,
        },
      })
    }

    parts.push({ text: prompt })

    const response = await genAI.models.generateContent({
      model: MODEL_NAME,
      contents: [{ role: 'user', parts }],
      config,
    })

    const responseText = response.text || ''
    const result = JSON.parse(responseText) as {
      scenes: SceneOutput[]
      visualStyle?: string
      narrativeFlow?: string
    }

    return NextResponse.json({
      scenes: result.scenes,
      visualStyle: result.visualStyle,
      narrativeFlow: result.narrativeFlow,
    })
  } catch (error) {
    console.error('ë©€í‹°ì”¬ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì˜¤ë¥˜:', error)
    return NextResponse.json(
      { error: 'Failed to generate multi-scene scenario' },
      { status: 500 }
    )
  }
}

function buildMultiScenePrompt(
  productName: string,
  productDescription: string | null | undefined,
  sellingPoints: string[] | null | undefined,
  sceneElements: SceneElementOptions[],
  overallMood: string | undefined,
  scenarioDescription: string | undefined,
  sceneCount: number,
  avgDuration: number
): string {
  // ì…€ë§ í¬ì¸íŠ¸ í¬ë§·íŒ…
  const sellingPointsText = sellingPoints && sellingPoints.length > 0
    ? sellingPoints.map((p, i) => `  ${i + 1}. ${p}`).join('\n')
    : 'Not provided'

  // ì”¬ë³„ ìš”ì†Œ ì„¤ëª… ìƒì„±
  const sceneElementsDescription = sceneElements.map((elem, idx) => `
=== SCENE ${idx + 1} SPECIFIC ELEMENTS ===
- Background: ${elem.background || 'Clean seamless backdrop'}
- Mood: ${elem.mood || overallMood || 'Premium commercial'}
- Composition/Angle: ${elem.cameraAngle || 'Cinematic'}
- Product Placement: ${elem.productPlacement || 'Center hero'}
- Lighting Effect: ${elem.lighting || 'Professional even lighting'}
- Color Tone: ${elem.colorTone || 'Natural'}
`).join('\n')

  // ëŒ€í‘œ ìƒ‰ìƒ í†¤ (ì¼ê´€ì„± ìœ ì§€ìš©)
  const primaryColorTone = sceneElements[0]?.colorTone || overallMood || 'cinematic'

  return `You are an expert advertising video director creating a PREMIUM AD CAMPAIGN with ${sceneCount} scenes.

ğŸš¨ğŸš¨ğŸš¨ READ FIRST - ABSOLUTE FORBIDDEN WORDS ğŸš¨ğŸš¨ğŸš¨
These words will cause actual cameras/equipment to appear in generated videos:
âŒ BANNED: "camera", "Camera", "tripod", "DSLR", "mirrorless", "lens" (except "lens flare")
âŒ BANNED: "photographer", "filming", "behind the scenes", "photo shoot", "studio setup"
âŒ BANNED: The product name "${productName}" - may contain misleading words like "Camera"
âœ… USE ONLY: "the product", "the item", "the bottle", "the package"

ğŸ¬ GOAL: Create ${sceneCount} SCENES where EACH scene uses its OWN specific elements listed below.

=== CRITICAL RULES ===
âœ… HUMAN ELEMENTS ALLOWED (when relevant):
- âŒ NO FULL FACE CLOSE-UPS (AI-generated faces often look unnatural)
- âœ… ALLOWED: Hands holding/using the product, partial body, silhouettes, back view
- âœ… ALLOWED: Lifestyle scenes with people (but keep faces obscured, out of frame, or from behind)

âŒ ABSOLUTELY NO VISIBLE PRODUCTION EQUIPMENT:
- NO cameras, tripods, lighting rigs, softboxes, ring lights, reflectors
- Describe lighting as EFFECT only (e.g., "soft highlights", "dramatic shadows")
- The video should look like a FINAL ADVERTISEMENT

âœ… OUTPUT LANGUAGE:
- scenePrompt: English only (for AI video generation)
- visualStyle: English (shared style description)
- narrativeFlow: Korean (for user)

â­ MUST REFERENCE THE ATTACHED IMAGE:
- Each scenePrompt MUST start with "The product shown in the attached image" or similar
- This ensures the AI uses the EXACT product appearance

=== PRODUCT INFORMATION ===
Product: [Refer to the product shown in the attached image]
Product Description: ${productDescription || 'Not provided'}

âš ï¸ WARNING: Do NOT include the product name "${productName}" directly in the generated prompts.
The product name may contain misleading words (e.g., "Camera Lens Cleaner" would generate actual cameras).
Instead, use generic terms like "the product", "the item" based on the attached image.
Product Selling Points:
${sellingPointsText}
Product Image: [ATTACHED - This is the PRODUCT to feature]

=== OVERALL MOOD (ì „ì²´ í†¤ì•¤ë§¤ë„ˆ) ===
${overallMood || 'Premium commercial style'}
${scenarioDescription ? `Scenario Description: ${scenarioDescription}` : ''}

=== âš ï¸ SCENE-SPECIFIC ELEMENTS (ê° ì”¬ë³„ ê°œë³„ ì„¤ì • - ë§¤ìš° ì¤‘ìš”!) ===
EACH SCENE HAS DIFFERENT SETTINGS. You MUST use the specific elements for each scene:

${sceneElementsDescription}

âš ï¸ IMPORTANT:
- Scene 1 MUST use Scene 1's background, lighting, composition, etc.
- Scene 2 MUST use Scene 2's settings, which may be DIFFERENT from Scene 1
- And so on for all scenes
- This creates visual variety while maintaining the overall mood

=== ğŸ¨ VISUAL CONSISTENCY & NARRATIVE FLOW (í†¤ì•¤ë§¤ë„ˆ + ìŠ¤í† ë¦¬ ì—°ê²°) ===
While scenes have DIFFERENT settings, they MUST tell a CONNECTED STORY:
1. **Narrative arc**: Opening (grab attention) â†’ Build interest â†’ Climax (memorable ending)
2. **Same overall mood/feeling**: ${overallMood || 'Premium commercial'}
3. **Visual continuity**: Consistent color palette, lighting mood, recurring visual motifs
4. **Logical flow**: Each scene should naturally lead to the next - NOT disconnected shots
5. **Be bold**: Match visuals to product personality (luxury=elegant, sports=dynamic, tech=futuristic)

=== SCENE PROMPT STRUCTURE (50-100 words) ===
Each scenePrompt MUST:
1. START by identifying the product: "The product shown in the attached image"
2. Use THAT SCENE's specific background, lighting, composition from the elements above
3. Be UNIQUE based on its specific elements
4. End with: "[THAT SCENE'S COLOR TONE] tones, soft professional lighting, photorealistic, 4K"

âš ï¸ FORBIDDEN WORDS (ì´ ë‹¨ì–´ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ë©´ ì´¬ì˜ ì¥ë¹„ê°€ ì˜ìƒì— ë“±ì¥í•¨!):
NEVER include: "camera", "tripod", "photographer", "filming", "behind the scenes", "DSLR", "mirrorless"

=== VIDU Q2 VIDEO AI RULES ===
- NO text/label rendering expectations
- Simple surface descriptions (no contradictions)
- ALL motion MUST include "slowly"
- Maximum 2 visual effects per scene

=== ğŸšï¸ MOVEMENT AMPLITUDE ===
- "small": Static, elegant, gimbal-stabilized (RECOMMENDED for professional look)
- "medium": Moderate motion with stability
- "large": Dynamic, energetic (use sparingly - opening/closing only)

=== ğŸ¥ CAMERA STABILIZATION (ë§¤ìš° ì¤‘ìš”!) ===
- Include "steady", "stable", "gimbal-stabilized" in prompts for smooth motion
- Add "no camera shake", "professional dolly motion" for stability
- AVOID "handheld", "shaky" - these cause unstable, amateur-looking footage
- Default to "small" movementAmplitude for broadcast-quality stability

=== DURATION ===
Each scene: ${avgDuration} seconds (recommended: 1-3 seconds, total video 6-15 seconds)

Generate ${sceneCount} CONNECTED scene prompts for "${productName}".
Create a unique and creative narrative flow that best highlights this specific product's characteristics.

âš ï¸ MANDATORY:
1. Each scene MUST use ITS OWN specific elements from the list above
2. Scene 1 uses Scene 1 elements, Scene 2 uses Scene 2 elements, etc.
3. Scenes should flow together as one story while having VISUAL VARIETY
4. HUMAN ELEMENTS ALLOWED: Hands/body/silhouettes OK, but NO full face close-ups
5. NO VISIBLE STUDIO EQUIPMENT - no cameras, tripods, lighting rigs, softboxes
6. Each prompt ends with that scene's color tone + "soft professional lighting, photorealistic, 4K"
7. End visualStyle with: "Overall mood: ${primaryColorTone}, premium commercial quality, no visible production equipment"`
}
